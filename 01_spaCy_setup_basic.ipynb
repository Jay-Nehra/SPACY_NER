{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tokenization\n",
    "\n",
    "Tokenization in NLP refers to when we want to break a text into individual components. This is one form of tokenization known as word tokenization. There are, however, many other forms, such as sentence tokenization. **Sentence tokenization** is precisely the same as word tokenization, except instead of breaking a text up into individual word and punctuation components, we break a text up into individual sentences.\n",
    "\n",
    "If you are familiar with Python, you may be familiar with the built-in split() function which allows for a programmer to split a text by whitespace (default) or by passing an argument of a string to define where to split a text, i.e. split(\".\"). A common practice (without NLP frameworks) is to split a text into sentences by simply using the split function, but this is ill-advised. Let us consider the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Mary J. Watson is known for his writing skills. She is also a good dancer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mary J', ' Watson is known for his writing skills', ' She is also a good dancer', '']\n"
     ]
    }
   ],
   "source": [
    "#Now, let's try and use the split function to split the text object based on punctuation.\n",
    "new = text.split(\".\")\n",
    "print (new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see than we have split the name in the first sentance along with splitting both of the sentances. The very thing that makes texts easier to read, however, greatly hinders our ability to easily split sentences. \n",
    "\n",
    "For this reason, another method is needed. This is where sentence tokenization comes into play. In order to see how sentence tokenization differs, let's begin with our first spaCy usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my use case, I will be using these setup commands, we can find the appropriate set up commands from the SpaCy page itself by entering the platform and use case details :\n",
    "[SpaCy Installation](https://spacy.io/usage)\n",
    "```\n",
    "pip install -U pip setuptools wheel\n",
    "pip install -U 'spacy[transformers,lookups]'\n",
    "python -m spacy download en_core_web_sm```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll begin with SpaCy and see how we can perform Tokenization. Next, we need to load an NLP model object.To do this, we use the spacy.load() function.\n",
    "This will take one argument, the model one wishes to load. We will use the small English model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the nlp object created, we can use it to to parse a text.\n",
    "To do this, we create a doc object.\n",
    "This object will contain a lot of data on the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary J. Watson is known for his writing skills. She is also a good dancer.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m           Doc\n",
      "\u001b[0;31mString form:\u001b[0m    Mary J. Watson is known for his writing skills. She is also a good dancer.\n",
      "\u001b[0;31mLength:\u001b[0m         17\n",
      "\u001b[0;31mFile:\u001b[0m           ~/.local/lib/python3.10/site-packages/spacy/tokens/doc.cpython-310-x86_64-linux-gnu.so\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Doc(Vocab vocab, words=None, spaces=None, user_data=None, *, tags=None, pos=None, morphs=None, lemmas=None, heads=None, deps=None, sent_starts=None, ents=None)\n",
      "A sequence of Token objects. Access sentences and named entities, export\n",
      "    annotations to numpy arrays, losslessly serialize to compressed binary\n",
      "    strings. The `Doc` object holds an array of `TokenC` structs. The\n",
      "    Python-level `Token` and `Span` objects are views of this array, i.e.\n",
      "    they don't own the data themselves.\n",
      "\n",
      "    EXAMPLE:\n",
      "        Construction 1\n",
      "        >>> doc = nlp(u'Some text')\n",
      "\n",
      "        Construction 2\n",
      "        >>> from spacy.tokens import Doc\n",
      "        >>> doc = Doc(nlp.vocab, words=[\"hello\", \"world\", \"!\"], spaces=[True, False, False])\n",
      "\n",
      "    DOCS: https://spacy.io/api/doc\n",
      "    \n",
      "\u001b[0;31mInit docstring:\u001b[0m\n",
      "Create a Doc object.\n",
      "\n",
      "vocab (Vocab): A vocabulary object, which must match any models you\n",
      "    want to use (e.g. tokenizer, parser, entity recognizer).\n",
      "words (Optional[List[Union[str, int]]]): A list of unicode strings or\n",
      "    hash values to add to the document as words. If `None`, defaults to\n",
      "    empty list.\n",
      "spaces (Optional[List[bool]]): A list of boolean values, of the same\n",
      "    length as `words`. `True` means that the word is followed by a space,\n",
      "    `False` means it is not. If `None`, defaults to `[True]*len(words)`\n",
      "user_data (dict or None): Optional extra data to attach to the Doc.\n",
      "tags (Optional[List[str]]): A list of unicode strings, of the same\n",
      "    length as words, to assign as token.tag. Defaults to None.\n",
      "pos (Optional[List[str]]): A list of unicode strings, of the same\n",
      "    length as words, to assign as token.pos. Defaults to None.\n",
      "morphs (Optional[List[str]]): A list of unicode strings, of the same\n",
      "    length as words, to assign as token.morph. Defaults to None.\n",
      "lemmas (Optional[List[str]]): A list of unicode strings, of the same\n",
      "    length as words, to assign as token.lemma. Defaults to None.\n",
      "heads (Optional[List[int]]): A list of values, of the same length as\n",
      "    words, to assign as heads. Head indices are the position of the\n",
      "    head in the doc. Defaults to None.\n",
      "deps (Optional[List[str]]): A list of unicode strings, of the same\n",
      "    length as words, to assign as token.dep. Defaults to None.\n",
      "sent_starts (Optional[List[Union[bool, int, None]]]): A list of values, \n",
      "    of the same length as words, to assign as token.is_sent_start. Will \n",
      "    be overridden by heads if heads is provided. Defaults to None.\n",
      "ents (Optional[List[str]]): A list of unicode strings, of the same\n",
      "    length as words, as IOB tags to assign as token.ent_iob and\n",
      "    token.ent_type. Defaults to None.\n",
      "\n",
      "DOCS: https://spacy.io/api/doc#init"
     ]
    }
   ],
   "source": [
    "doc?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this looks identical to the \"text\" string above, it is quite different. To demonstrate this, let us use the sentence tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary J. Watson is known for his writing skills.\n",
      "She is also a good dancer.\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print (sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have used the spaCy sentence tokenizer to generate a desired output: a text correctly broken into sentences. This shows why using an NLP framework for performing even a basic task is not only easier, but essential. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition \n",
    "\n",
    "Objective of this series, is named entity recognition (NER). Here, I'd like to demonstrate how to perform basic NER via spaCy. Again, we will iterate over the doc object as we did above, but instead of iterating over doc.sents, we will iterate over doc.ents. For our purposes right now, I simply want to print off each entity's text (the string itself) and its corresponding label (note the _ after label). I will be explaining this process in much greater detail later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary J. Watson PERSON\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts Of Speech \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see two vital pieces of information: the string and the corresponding part-of-speech (pos). For a complete list of the pos labels, see the spaCy documentation (https://spacy.io/api/annotation#pos-tagging). \n",
    "Here, PROPN is proper noun, AUX is an auxiliary verb, ADJ, is adjective, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary PROPN\n",
      "J. PROPN\n",
      "Watson PROPN\n",
      "is AUX\n",
      "known VERB\n",
      "for ADP\n",
      "his PRON\n",
      "writing NOUN\n",
      "skills NOUN\n",
      ". PUNCT\n",
      "She PRON\n",
      "is AUX\n",
      "also ADV\n",
      "a DET\n",
      "good ADJ\n",
      "dancer NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Nouns and Noun Chunks\n",
    "Often times when working with a text, we need to extract nouns and noun chunks. There are a few different ways that we can do this via spaCy. To extract nouns, we can use the doc.noun_chunks attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary J. Watson\n",
      "his writing skills\n",
      "She\n",
      "a good dancer\n"
     ]
    }
   ],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Verbs and Verb Phrases\n",
    "In order to extract all verbs, we can leverage the POS tagger's output in spaCy. We can establish a for loop to iterate over all POS tags in the doc object and then print off just the ones that are either a \"VERB\" or \"AUX\". These are the two POS tags used to identify tokens in a sentence that function as verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is AUX\n",
      "known VERB\n",
      "is AUX\n"
     ]
    }
   ],
   "source": [
    "verbs = [\"VERB\", \"AUX\"]\n",
    "for token in doc:\n",
    "    if token.pos_ in verbs:\n",
    "        print (token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "**Lemmatization** is an essential component in most NLP frameworks, though some libraries perform this concept differently. While libraries, such as Stanza will find word stems, spaCy will find word lemmas. They are technically a little different, but both seek to reduce all words to their roots. To find lemmas via spaCy, we use the same process as we did for finding a word's part of speech, via iterating over the tokens in the doc object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary Mary\n",
      "J. J.\n",
      "Watson Watson\n",
      "is be\n",
      "known know\n",
      "for for\n",
      "his his\n",
      "writing writing\n",
      "skills skill\n",
      ". .\n",
      "She she\n",
      "is be\n",
      "also also\n",
      "a a\n",
      "good good\n",
      "dancer dancer\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we see most words remain the same, but notice particularly \"is\" being identified as \"be\" and \"known\" becomes \"know\". These are the respective lemmas for these verbs. Also notice the same effect on nouns, such as \"skills\", a plural, being reduced to \"skill\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
